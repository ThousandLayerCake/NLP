# 背景

## OK-VQA任务背景

现有VQA任务的基准模型在简单的识别问题上表现很好，但是在需要 ==**常识**== 和 ==**超出给定图像区域的领域知识**== 的问题上却表现不好。



因此，**提出了基于外部知识的VQA任务（OK-VQA）**，作为理解开放式场景的一种代理方法，只有当一个人检索到相关的知识，才能回答这一类问题。



### 例子

![image-20230914165945551](https://cdn.jsdelivr.net/gh/ThousandLayerCake/picbed/image-20230914165945551.png)

下面举一个例子：

- 问题：为什么大象已经濒临灭绝？
- 图像：图1

传统的VQA回答这个问题是很难的，**但当有外部知识：==偷猎者为猎取象牙而捕猎==** ，回答这个问题才会变得容易一些。因此，相关的知识的检索，对于这类问题的回答至关重要。





### 知识

知识分为显式知识和隐式知识，下面介绍一下两种知识以及对VQA任务的影响：



#### 显式知识

**显式知识**，本质上是一个大容量的数据集，在回答问题的时候的一种补充线索，它可以被理解为对问题的证据，即它可以作为回答问题的依据。

现有模型是通过把显式知识作为训练任务的一个补充。但是这会受到噪音融入和错误传播的影响，因为它们没有判断该显式知识是否有助于问题的解决，即它对问题的回答是否起到了帮助作用。



**在模型训练方面，输入数据的架构如下**：

- 问题分支
- 图像分支
- 知识分支



**不足：**

1. 导致更多的 **==噪声==** 和额外的 **==检索知识的开销==**
2. 由于 **==对知识检索没有监督==**，因此当检索的知识是不能作为回答问题的依据时，**==经验误差会被传播==**。



#### 隐式知识

**隐式知识**，隐藏在大规模与训练语言模型中的知识。就是存在于文本、图像和其他模态数据中，但是不容易直接提取或明确表示的知识。



**不足：**

1. 现有模型对这类知识仍然有==**大部分未被充分探索，未充分使用**==。





## 存在的问题

**本文主要为了解决当前最先进方法的两个问题：**

1. 显式知识的引入导致的噪声和错误传播问题
2. 隐式知识的未充分利用问题



**具体来说：**

1. 显式知识方面：将显式外部知识作为VQA模型的补充，以弥补模型训练时的不足，尽管方法有效，但是 ==这些方法常常受到噪音融入和错误传播的影响。==
2. 隐式知识方面：存在于文本、图像和其他模态数据中，但是不容易直接提取或明确表示的知识。==这类知识在知识驱动的视觉问答（VQA）任务中仍然有大部分未被充分探索。==





## 解决思路

本文提出了一个统一的端到端的 **Retriever-Reader** 框架，用于基于知识的VQA任务。



**针对在显式知识上遇到的检索操作中的噪声问题1：**

- 我们设计了一种方案来创建伪标签，以实施有效的知识监督 Retriever。
  - 使得 **Retriever** 可学习
  - 保留对问题有用的显式知识
  - 丢弃对问题无用的显式知识





**针对隐式知识未被充分利用的问题2：**

- 使用大的预训练模型 （如**LXMERT**） 来==初始化多模态融合的参数==，以便充分使用多模态隐性知识















# 模型

本文提出了一个统一的端到端的 **Retriever-Reader** 框架，用于OK-VQA任务。

![image-20230914165945551](https://cdn.jsdelivr.net/gh/ThousandLayerCake/picbed/image-20230914165945551.png)

图1：UnifER模型示例图





## 模型概述

如图1所示，模型分为 **Retriever** 和 **Reader** 两个组件，两个组件都是由 **Transformer Encoder** 这一个基础组件构成。 



### 前向传播

（1）输入数据\<图像，问题，显式知识条目>转换为句子，通过Transformer编码器进行嵌入，然后进行预先定义的 **相似性函数（余弦相似度）**，检索最相关的显式知识条目。（**检索器Retriever**）。

（2）检索到==最相关的显式知识条目后==，UnifER 使用 **阅读器Reader** 进行答案预测。





### 解决问题的核心思路 【核心创新】

为了解决 ==显式知识引起的噪声和错误传播问题==，设计了一种方案使得：对检索器进行相关性标注（也就是对检索器的检索做一个评价指标），对阅读器进行实例选择（选择最相关的显式知识）。



**详细设计：**

- 设计常规VQA的任务损失$L_{QI}$

- 设计带外部知识的VQA的任务损失$L_{QIK}$

将两个损失的差值定义为：
$$
𝛿=L_{QI} - L_{QIK}
$$
当$𝛿>0$时，即常规VQA任务的损失大于带外部知识的VQA任务损失，因此我们可以假设：==检索到的知识，对于预测答案至少产生了积极的影响==。因此，这个差值对于检索器和阅读器都产生了显著的影响，对于检索器可以检索到有帮助的显式知识，对于阅读器可以选择更好的显示知识实例。



# 方法

## 问题定义

开放式场景下的通用视觉问答 **OK-VQA** 任务，它是指给定一个关于图像 $I$ 的文本问题 $Q$，**OK-VQA** 的预测目标可以用公式表示为：
$$
\hat{a} = argmax_{a \in \Omega}\ \ p(a|Q,I;\Theta) \tag{1}
$$
其中，$\Omega$和$\Theta$分别表示 真实的答案集合 和 模型参数。

注意，隐式知识可以通过$\Theta$进行表达，**通常$\Theta$是在其他大规模的数据集上进行了广泛的预训练**，并使用了一些专门的预训练任务。也正是基于此，==**作者认为通过使用预训练好的大的VL模型，就可以使得隐式知识可以充分利用。**==
$$
p(a|Q,I,K;\Theta) = \underbrace{p(K_{ret}|Q,I,K;\Phi)}_{检索器} ·
\underbrace{p(a|Q,I,K_{ret};\Theta)}_{阅读器} \tag{2}
$$
其中，$K$和$K_{ret}$分别表示**整体广泛的外部知识数据集**和**检索到的相关子集**。$\Phi$表示检索步骤中模型的参数，也就是检索器的参数。**我们将整个OK-VQA模型分为检索器和阅读器。**前者从$K$中检索知识，后者利用检索到的相关知识$K_{ret}$进行答案预测。

检索器和阅读器都是基于基础的**Transformer Encoder**。因此，我们先介绍**Transformer Encoder**



### Knowledge-based VQA 的基础编码器

![image-20230914225311050](https://cdn.jsdelivr.net/gh/ThousandLayerCake/picbed/image-20230914225311050.png)

图2：基于Transformer的基础编码器

首先介绍这个基础编码器结构，如图2所示，它是UnifER中检索器和阅读器的重要组成部分。基础编码器受到最近流行的 ==**基于Transformer的视觉-语言模型**== 的启发。

**输入处理：**

基础编码器有三种类型的输入：知识条目、问题和图像。

以下是对输入的具体处理：

- **知识条目**：对于每个 **结构化的知识条目（三元组）**，例如 <tusk, related to, weapon>，我们将三元组转换为一个文本句子“tusk related to weapon”，以便能够顺利地通过Transformer编码器进行处理。[str]

- **问题**：字符串类型，不处理，与知识条目字符串进行拼接。[str]

- **图像输入**：每个图像可以被划分为32×32的图块，即$V \in R^{32×32×3}$，遵循ViT模型；或者我们可以使用区域底层特征作为原始输入，即$V \in R^{𝑚×𝑏}$，其中𝑚和𝑏分别表示提取的视觉特征的维度和检测到的对象的数量。`【论文出处：Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering.】`

  



**模态嵌入：**

基于处理后的知识条目，我们在知识句子$T_k$中添加两个特殊标记。
$$
T=[CLS] ⊕ T_K ⊕ [SEP]
$$
其中，⊕表示拼接操作，[𝐶𝐿𝑆]和[𝑆𝐸𝑃]分别是 BERT 原始设计的分类标记和分隔标记。然后，我们将文本问题$𝑇_𝑄$追加到$𝑇$中，表示为：
$$
T=T ⊕ T_Q ⊕ [SEP]
$$
然后，我们将文本$𝑇$ 输入到预训练的查找表中（**词表**），并获得其词嵌入$E_𝑤 ∈ R^{(𝑛+𝑘)×𝑑}$，其中$𝑛 + 𝑘$是标记的总数，$𝑑$表示嵌入的维度。同时，对于图像输入$V$，我们将其转换到与文本$𝑇$ 相同的空间中（$dim=1$ 维度相同），得到$E_𝑣 \in R^{𝑚×𝑑}$，并将其与$E_𝑤$连接起来得到$E_e \in R^{(𝑛+𝑘+𝑚)×𝑑}$。

在这里，获得了 **文本+知识条目、图像** 的嵌入Embedding

除了 **token/图像嵌入** 之外，我们还可以使用位置嵌入$E_𝑝 \in R^{(𝑛+𝑘+𝑚)×𝑑}$，它捕捉了输入的顺序属性。需要注意的是，图像的位置标记可以是静态的，因为图像没有顺序模式。此外，引入了一个段落嵌入$E_𝑠 \in R^{(𝑛+𝑘+𝑚)×𝑑}$，用于指示其为图像嵌入与文本嵌入相对应。最后，我们将这三个嵌入组合成一个单一的嵌入：
$$
E = E_e + E_p + E_s \tag{3}
$$





**模态融合：**

Transformer在多模态融合方面取得了成功，这与传统的加法或连接技术有所区别。传统的技术是将两个模态的嵌入通过相加或者简单的拼接来实现，而Transformer在多模态融合上大致分为两种：**双流和单流**。

- 双流方法：利用独立的Transformer对每个模态进行内部融合，然后再将多个模态输入到另一个Transofmer进行融合，用公式表示：

$$
E_1 = Trans_1(X_1),E_2 = Trans_2(X_2),...,E_n = Trans_n(X_n)\\
E_{融合} = Trans(E_1, E_2,...,E_n)
$$

- 单流方法：利用早期融合策略将不同模态的数据融合在一起。



接下来，我们以 **单流方法** 为例，来说明模态融合的机制。

基础编码器是由多个堆叠的Transformer块组成，每个块包括一个 多头自注意力（MSA）层 和 一个多层感知机（MLP）层。此外，每个层还涉及层归一化（LN）操作和残差连接，编码器的第$l$个块可以表示为：
$$
\left\{ 
\begin{matrix}
E^0 = E, \\
\hat{E^l} = MSA(LN(E^{l-1})) + E^{l-1}  \\
E^l = MLP(LN(\hat{E^l})) + \hat{E^l} \\
\end{matrix}
\right.

\tag{4}
$$
其中，加法代表残差连接。之后我们采用最后一个块L的输出进行进一步估计。
$$
o = MLP(E^L_{CLS}) \tag{5}
$$
之中，$E^L_{CLS}$表示来自$[CLS]$标记位置的输出。就是标准的BERT的输出操作，使用[CLS]位置的token进行结果预测。为了下面方便表示，我们使用一个单一函数来表示上述过程：
$$
o=f^{be}(Q,I,K;\Psi)\tag{6}
$$
其中，**$\Psi$表示所有相关的参数。**具体如图$2(a)$所示。

关于基础编码器，由三个值得注意的特点：

1）对于一个模型，并不一定需要这三个模态的输入。可以只输入其中任意个的模态。

2）具有多模态输入的基础编码器，它的参数是使用 视觉-语言预训练模型（例如 ViLT） 进行初始化的。这样，多模态的隐式知识就可以无缝地融合，使得其充分利用。对于仅包含文本的输入，我们使用预训练的 Roberta 模型作为基础编码器。【多模态任务：偷ViLT；NLP任务：偷Roberta】

3）基础编码器同时是 **检索器** 和 **阅读器** 的重要组成部分。



### 检索器

我们使用深度神经网络对多模态数据进行特征提取，实现知识检索。每个 **知识条目$K$** 和 **问题-图像对$QI$** 都分别使用基础编码器进行编码，用公式表示如下：
$$
\left\{ 
\begin{matrix}
h_{QI} = f^{be}(Q,I;\Phi_{QI}) \\
h_{K_i} = f^{be}(K_i;\Phi_{K}) \\
\end{matrix}
\right.\tag{7}
$$
这里使用了公式$(6)$的函数，其中$\Phi_{QI}$和$\Phi_{K}$分别表示 **问题-图像分支** 和 **知识分支** 的参数，$f^{be}$是上述模态融合的过程。具体如图$2(b)$所示。



然后我们使用 ==**余弦相似度**== 函数来估计 **每个知识条目$𝐾_𝑖$与给定的图像-问题对𝑄𝐼之间的相似度**：
$$
Sim(QI,K_i) = \frac{h^T_{QI}h_{K_i}}{||h_{QI}||_2·||h_{K_i}||_2} \tag{8}
$$
其中，这个函数其实就是两个向量之间求得$cos$值的函数，即$Sim = cos$，取值范围$[-1,+1]$。

当$Sim=1$时，说明两个向量的向量角为0度，两向量重合，因此来表示知识条目$K_i$与问题图像对$QI$最相似。



我们在图$2(b)$中说明了Retriever的流程，其中𝐾和𝑄𝐼输入分别被屏蔽。根据公式$(8)$，我们可以从整个知识集K中检索出与给定图像-问题对最相关的子集$K_{𝑟𝑒𝑡} = {𝐾_1, 𝐾_2, ..𝐾_𝑡}$。

需要注意的是，知识检索没有监督，可以认为$K_{𝑟𝑒𝑡}$在模型训练过程中引入了噪声。例如，图$3$中的两个知识条目，即 <前院，有，草地> 和 <大象，是，哺乳动物> 与图1中的问题 <是什么让这些动物濒临灭绝？> 不相关。【体现在公式上就是这两条知识条目与问题-图像的Sim值可能偏向于-1】

![image-20230915105807806](https://cdn.jsdelivr.net/gh/ThousandLayerCake/picbed/image-20230915105807806.png)



### 阅读器

在检索器检索到所需要的知识$K_{ret}$后，阅读器要做的就是将检索到的知识$K_{ret}$、问题和图像输入到基础编码器进行模态融合，为了实现这一个目标，我们将 **融合的多模态特征** 映射到 答案空间，因为当前的VQA模型都将VQA视为分类问题，因此我们需要一个分类器MLP，即：
$$
\hat{y} = MLP(f^{be}(Q,I,K_i;\Theta))\tag{9}
$$
其中，$\hat{y}$的长度等于答案集合$\Omega$的大小。然后，我们使用 **交叉熵损失** 对模型参数进行优化：
$$
L_{QIK}(Q,I,K_i;\Theta) = \sum_{j=1}^{|\Omega|}-y_ilog\hat{y}_j\tag{10}
$$
其中，$y_j$表示真实的标签，大多数方法优化函数都会使用类似公式$10$这样的损失函数，并致力于整合多种外部知识源。然而，我们认为这种操作会引发两个问题：**==检索开销大 和 错误传播==**。

- 对于检索开销大：因为知识源是无限的，因此更多知识的引入必然导致检索负担的增加。
- 对于错误传播：如果检索到的知识与问题不相关，例如图3中的 <前院，有，草地>，则在这种情况下引入的知识实际上产生了<u>负面效果</u>。

因此，当使用公式$10$中的答案预测损失进行优化时，经验误差将进一步加剧。解决这样的问题并不简单，因为 “正确的知识” 对于模型来说是未知的（因为相关的外部知识数据集是没有标注标明该知识适用于什么问题的）。基于此，我们提出通过一种新的监督训练方案来解决上述两个问题，将在 **优化方法** 这一节解释。



### 优化方法 【核心创新】



#### 阅读器损失 【核心创新】

在第一步中，我们使用 图像-问题 输入计算损失，并估计该损失$L_{QI}$与使用检索到的知识的损失$L_{QIK}$之间的数学差异，称为损失差距𝛿：
$$
\left\{ 
\begin{matrix}
\overline{y} = MLP(f^{be}(Q,I;\Theta)),\\
L_{QI}(Q,I;\Theta) = \sum_{j=1}^{|\Omega|}-y_ilog\overline{y}_i \\
𝛿 = L_{QI} - L_{QIK} \\
\end{matrix}
\right.
\tag{11}
$$
根据公式$11$，如果$𝛿<0$，表示$L_{QI}<L_{QIK}$，也就是意味着引入了外部知识，但是导致损失进一步上升，==因此我们可以假定输入的知识变成了“噪声”==。知识对问题回答产生了负面影响，为了避免错误传播，我们应该忽略它们进行优化。基于这一点，我们在公式$10$的损失函数中分配了一个正则化权重，得到如下的损失函数表达形式：
$$
L_{QIK}(Q,I,K_i;\Theta) = \sigma(\delta)*\sum_{j=1}^{|\Omega|}-y_ilog\hat{y}_j \tag{12}
$$
其中，$\sigma(·)$是sigmoid函数，取值范围$[-1,+1]$。是一个正则化项，用于平滑损失差距。

- 当$\delta$沿着$-\infin$方向移动时，
  - $\sigma(\delta)$的值趋于$0$，不至于使得损失变得巨大，无法计算。
  - 当损失函数的值很大，说明需要这次的预测结果很差，需要进行优化。当损失函数的值很小[=0]，说明这次的预测结果很好，不需要进行优化。
  - 我们的目标是，当$\delta$很小时，为了避免错误传播，应该不让它优化。刚好这个正则化项$\sigma({\delta})$会趋于0，使得损失函数值变得很小，从而使得不优化。



**小结：**基于 <图像，问题> 的损失函数$L_{QI}$，与基于 <知识，图像，问题> 的损失函数$L_{QIK}$作差，==得到的 𝛿 用来衡量知识是否起到了作用==，效果上来说，通过该$\delta$的值，来决定是否进行优化，以决定是否采用该知识。





#### 检索器损失 【核心创新】

##### 难点

对于检索器来说，判断检索到的知识是否正确是不现实的，因为基准数据集中没有标签标注说明该知识是否适应哪些问题。【都是无监督数据集】

因此，我们设计了一个伪标签，来监督检索器的检索结果。



##### 做法

为了实现对检索器的检索结果进行监督，我们将公式$(11)$定义的 **损失差距𝛿** 重新用作知识检索的指示。

具体来说，我们使用$tanh$函数将$\delta$做一次变换使其落入$[-1,+1]$，即$tanh(\delta) \in [-1, +1]$，以符合相似性区间，由于$\delta$越大，说明检索的知识越有帮助，因此我们假定：

- 最优值 $1.0$ 表示：检索到的知识与给定的图像-问题对完全匹配
- 最差值 $-1.0$ 表示：检索到的知识与给定的图像-问题对完全不相关



**因此，我们设计了一个检索器损失 $L_{RET}$ 来实现这个想法：**
$$
L_{RET}(Q,I,K;\Theta) = \frac{1}{t}\sum_{i=1}^{t}(tanh(\delta) - Sim(QI,K_i))^2 \tag{13}
$$
其中$𝑡$表示检索到的知识条目的数量，相似性函数$Sim$可以参考公式8。最终，总损失是检索器损失$L_{𝑅𝐸𝑇}$和阅读器损失$L_{𝑄𝐼𝐾}$的线性组合：
$$
L = L_{QIK} + \lambda{L_{RET}} \tag{14}
$$
其中 $\lambda$ 是用于平衡这两个损失数值上差距的超参数。可以观察到，整个框架可以以端到端的方式进行训练。

在推理过程中，给定图像-问题对，我们首先通过检索器检索出最相关的知识条目。然后，我们将这个知识与图像和问题一起，通过阅读器进行答案预测。



##### 公式$13$ 分析

我们将$tanh(\delta)=1$表示为完全匹配，将$Sim(QI,K_i)=1$表示为$QI$与$K_i$相似，则会出现以下四种情况：

1. 完全匹配，$QI$与$K_i$相似，$L_{RET}=(1-1)^2=0$  
2. 完全匹配，$QI$与$K_i$不相似，$L_{RET}=(1-(-1))^2=4$ 
3. 完全不匹配，$QI$与$K_i$相似，$L_{RET}=((-1)-1)^2=4$
4. 完全不匹配，$QI$与$K_i$不相似，$$L_{RET}=((-1)-(-1))^2=0$$



- 针对情况1，检索器检索到很相关的知识$K_i$（$Sim=1$），并且知识$K_i$起到了作用（$tanh=1$），因此没有损失。
- 针对情况2，检索器检索到知识$K_i$，但认为它不相关（$Sim=-1$），但是知识$K_i$起到了作用（$tanh=1$），因此损失变大，惩罚检索器。
- 针对情况3，检索器检索到很相关的知识$K_i$（$Sim=1$），但是知识$K_i$起不了作用（$tanh=-1$），因此损失变大，惩罚检索器。
- 针对情况4，检索器检索到知识$K_i$，但认为它不相关（$Sim=-1$），并且知识$K_i$起不了作用（$tanh=-1$），因此没有损失。





# 总结

本文在基于外部知识的视觉问答任务（OK-VQA）的问题基础上，看到了目前OK-VQA模型存在的两个问题：(1) 显式知识的引入导致的噪声和错误传播问题；(2)隐式知识的未充分利用问题。进而针对问题1，作者**提出设计一种 虚拟标签 方案来使得显式知识引入的过程中受到 虚拟标签 的监督**，从而解决噪声和错误传播的问题；而针对问题2，作者基于主流的Transformer Encoder的视觉-语言模型的启发，通过 **移植其他优秀的大规模 VL 模型参数**，来充分挖掘隐式知识的潜力。作者设计了一个基于Transformer Encoder的 检索器-阅读器 框架，通过设计 **检索器损失函数、阅读器损失函数**，使得整个模型可以端到端的进行训练，并获得很好的效果。

