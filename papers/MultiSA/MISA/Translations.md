# 摘要

多模态情感分析是一项活跃的研究领域，利用多模态信号对用户生成的视频进行情感理解。解决这一任务的主要方法是开发复杂的融合技术。然而，这些信号的异构性质会导致分布模态差距，从而带来重大挑战。在本文中，我们旨在学习有效的模态表示以辅助融合过程。我们提出了一种新颖的框架，名为MISA，它将每个模态投影到两个不同的子空间中。第一个子空间是模态不变的，跨模态表示在其中学习它们的共同之处并减小模态差距。第二个子空间是模态特定的，它针对每个模态私有，并捕捉其特征性特点。这些表示提供了多模态数据的整体视图，用于融合以进行任务预测。我们在流行的情感分析基准数据集MOSI和MOSEI上进行了实验证明，相对于最先进的模型，我们取得了显著的改进。我们还考虑了多模态幽默检测任务，并在最近提出的UR_FUNNY数据集上进行了实验。同样，我们的模型表现优于强基线模型，证明了MISA作为一个有用的多模态框架。



**数据集：**

- UR_FUNNY





# 引言

随着用户生成的在线内容（如视频）的丰富，人类口语语言的多模态情感分析（MSA）已成为一个重要的研究领域[33, 45]。与传统的针对孤立模态（如文本、语音）进行的情感学习任务不同，多模态学习利用了包括语言（文本/转录/ASR）、音频/声学和视觉模态在内的多个信息源。MSA中的大多数方法都围绕着开发复杂的融合机制，从基于注意力的模型到基于张量的融合[41]。尽管取得了进展，但这些融合技术常常面临异构模态之间的模态差距的挑战。此外，我们希望融合互补信息以减少冗余并整合多样化的信息。辅助多模态融合的一种方式是首先学习捕捉这些理想属性的潜在模态表示。为此，我们提出了MISA，一种新颖的多模态框架，它为每个模态学习了分解的子空间，并将更好的表示作为融合的输入。
受到领域自适应[5]的最新进展的启发，MISA为每个模态学习了两种不同的话语表示。第一个表示是模态不变的，旨在减小模态差距。在这里，话语的所有模态都被映射到一个具有分布对齐的共享子空间中。尽管多模态信号来自不同的来源，但它们共享说话者的共同动机和目标，这是话语整体情感状态的原因。不变映射有助于捕捉这些潜在的共同点和相关特征，将它们作为对共享子空间上的对齐投影。大多数先前的工作在融合之前不利用这种对齐，这给它们的融合增加了额外的负担，以弥合模态差距并学习共同特征。除了不变子空间，MISA还学习了每个模态私有的模态特定特征。对于任何话语，每个模态都具有独特的特征，包括与说话者相关的风格信息。这种特殊的细节通常与其他模态不相关，并被归类为噪声。然而，它们在预测情感状态方面可能是有用的，例如，说话者倾向于使用讽刺或偏向情感极性的特殊表达方式。因此，学习这种模态特定特征补充了不变空间中捕捉的共同潜在特征，并提供了话语的全面多模态表示。我们建议使用这个完整的表示集进行融合（见图1）。

为了学习这些子空间，我们结合了多种损失函数，包括分布相似性损失（用于不变特征）、正交损失（用于特定特征）、重构损失（用于模态特征的代表性）和任务预测损失。我们在MSA的两个流行基准数据集MOSI和MOSEI上评估了我们的假设。我们还检查了我们的模型对另一个类似任务——多模态幽默检测（MHD）的适应性，其中我们评估了最近提出的UR_FUNNY数据集。在这三种情况下，我们观察到强大的改进，超过了最先进的模型，突显了MISA的有效性。
本文的创新贡献可以总结如下：
• 我们提出了MISA——一个简单而灵活的多模态学习框架，强调多模态表示学习作为多模态融合的前置条件。MISA学习模态不变和模态特定的表示，以全面且解耦的方式呈现多模态数据，从而有助于融合以预测情感状态。
• 在MSA和MHD任务上的实验证明了MISA的强大之处，学习到的表示帮助简单的融合策略超过了复杂的最先进模型。

![image-20231106100700946](https://cdn.jsdelivr.net/gh/ThousandLayerCake/picbed/image-20231106100700946.png)





# 方法

**3.1 任务设置**
我们的目标是通过利用多模态信号在视频中检测情感。数据中的每个视频都被分割成其组成的话语单元，其中每个话语单元本身都是一个较小的视频，被视为模型的输入。对于一个话语𝑈，输入由来自语言（𝑙）、视觉（𝑣）和声学（𝑎）模态的三个低级特征序列组成。它们分别表示为$U_𝑙 ∈ R^{𝑇_𝑙×𝑑_𝑙}$、 $U_v ∈ R^{𝑇_v×𝑑_v}$ 和$U_a ∈ R^{𝑇_a×𝑑_a}$。这里的$𝑇_m$表示话语的长度，比如标记数量（$𝑇_𝑙$），对于模态$𝑚$，$𝑑_𝑚$表示相应的特征维度。这些特征的详细信息将在第4.3节中讨论。

给定这些序列$$U_𝑚∈ {𝑙,𝑣,𝑎}$$，主要任务是预测话语𝑈的情感倾向，可以是预定义的𝐶个类别𝑦 ∈ R^C^，或作为连续的强度变量𝑦 ∈ R。



**3.2 MISA**
MISA的功能可以分为两个主要阶段：模态表示学习（第3.3节）和模态融合（第3.4节）。完整的框架如图2所示。



**3.3 模态表示学习**
话语级别的表示。首先，对于每个模态𝑚 ∈ {𝑙, 𝑣, 𝑎}，我们将其话语序列$U_𝑚 ∈ R^{𝑇_𝑚×𝑑_𝑚}$映射到一个固定大小的向量$u_𝑚 ∈ R^{d_h}$。我们使用堆叠的双向长短时记忆网络（LSTM）[20]，其最终状态的隐藏表示与一个全连接的稠密层结合，得到$u_𝑚$：
$$
u_m = sLSTM(U_m;\theta^{lstm}_m) \tag{1}
$$


模态不变和模态特定表示。现在，我们将每个话语向量$u_𝑚$投影到两个不同的表示。首先是模态不变组件，它在一个共享的子空间中学习一个共同的表示，具有分布相似性约束[18]。这个约束有助于最小化异质性差距，这是多模态融合的一个理想属性。第二个是模态特定组件，它捕捉该模态的独特特征。通过本文，我们认为同时存在模态不变和模态特定表示提供了所需的全面视角，用于有效的融合。学习这些表示是我们工作的主要目标。
给定模态𝑚的话语向量u𝑚，我们使用编码函数学习隐藏的模态不变表示 $h^c_m \in R^{d_h}$ 和模态特定表示 $h^p_m \in R^{d_h}$：
$$
h^c_m = E_c(u_m;\theta^c), h^p_m = E_p(u_m;\theta^p_m) \tag{2}
$$


为了生成六个隐藏向量$h^{𝑝/𝑐}_{𝑙/𝑣/𝑎}$（每个模态两个），我们使用简单的前馈神经网络层；$𝐸_𝑐$在所有三个模态上共享参数𝜃^𝑐^，而$𝐸_𝑝$为每个模态分配单独的参数$𝜃^𝑝_m$.



![image-20231106100753504](https://cdn.jsdelivr.net/gh/ThousandLayerCake/picbed/image-20231106100753504.png)





**3.4 模态融合**
在将模态投影到各自的表示之后，我们将它们融合成一个联合向量，用于下游的预测。我们设计了一个简单的融合机制，首先进行基于Transformer [54]的自注意操作，然后将所有六个转换后的模态向量进行串联。
定义Transformer。Transformer利用一个注意力模块，该模块定义为一个缩放的点积函数：
$$
Attention(Q, K, V) = softmax(\frac{QK^𝑇}{\sqrt{(d_h)}})V \tag{3}
$$




其中，Q、K和V分别是查询、键和值矩阵。Transformer计算多个并行的注意力，其中每个注意力输出被称为一个头（head）。第i个头计算如下：
$$
head_i = Attention(QW^q_i, KW^k_i, VW^v_i) \tag{4}
$$




$𝑊^{𝑞/𝑘/𝑣}_𝑖 ∈ R^{𝑑_ℎ×𝑑_ℎ}$是头部特定的参数，用于将矩阵线性投影到局部空间中。





**融合过程。**首先，我们将六种模态表示（根据公式（2））堆叠成一个矩阵$M = [h^𝑐_𝑙，h^𝑐_𝑣，h^𝑐_𝑎，h^𝑝_𝑙，h^𝑝_𝑣，h^𝑝_𝑎] ∈ R^{6×𝑑_ℎ}$。然后，我们对这些表示进行多头自注意操作，使每个向量能够感知与其相关的其他跨模态（和跨子空间）的表示。通过这样做，每个表示可以从相关的表示中提取对整体情感定向有协同作用的潜在信息。这种跨模态匹配在最近的跨模态学习方法[22，23，27，49，57]中非常重要。

对于自注意力机制，我们设置$Q = K = V = M ∈ R^{6×𝑑_ℎ}$。Transformer生成一个新的矩阵$\bar{M} = [\bar{h}^𝑐_𝑙，\bar{h}^𝑐_v，\bar{h}^𝑐_a，\bar{h}^p_𝑙，\bar{h}^p_v，\bar{h}^p_a]$，如下所示：
$$
\bar{M} =MultiHead(M; 𝜃^{𝑎𝑡𝑡}) = (head_1 ⊕ ... ⊕ head_𝑛)𝑊^𝑜 \tag{5}
$$



其中，每个$head_𝑖$根据公式（4）进行计算； ⊕表示连接操作；$𝜃^{𝑎𝑡𝑡} = {𝑊_𝑞，𝑊_𝑘，𝑊_𝑣，𝑊_𝑜}。$

预测/推断。最后，我们将Transformer的输出进行连接，构建一个联合向量$h^{𝑜𝑢𝑡} = [\bar{h}^𝑐_𝑙 ⊕ ... ⊕ \bar{h}^𝑝_𝑎] ∈ R^{6𝑑_ℎ}$。然后，任务的预测由函数$\hat{y} = 𝐺(h^{𝑜𝑢𝑡}; 𝜃^{𝑜𝑢𝑡})$生成。

我们在附录中提供了函数𝑠𝐿𝑆𝑇𝑀()，𝐸𝑐()，𝐸𝑝()，𝐺()和𝐷()的网络拓扑结构（稍后解释）。





**3.5 学习**

该模型的整体学习是通过最小化以下损失函数来完成的：
$$
L = L_{task} + 𝛼L_{sim} + 𝛽L_{diff} + 𝛾L_{recon} \tag{6}
$$



其中，𝛼、𝛽和𝛾是交互权重，用于确定每个正则化分量对总体损失L的贡献。

每个分量损失都负责实现所需的子空间属性。我们接下来讨论它们。



**3.5.1 $L_{sim}$ - 相似性损失。**

最小化相似性损失减小了每个模态的共享表示之间的差异。这有助于将共同的跨模态特征对齐到共享子空间中。在众多选择中，我们使用中心矩差异（Central Moment Discrepancy，CMD）[63]度量来实现此目的。CMD是一种先进的距离度量方法，通过匹配它们的逐阶矩差异来衡量两个表示的分布差异。直观地说，当两个分布变得更相似时，CMD距离会减小。



*CMD 的定义*。设𝑋和𝑌是区间[𝑎, 𝑏]上具有相应概率分布𝑝和𝑞的有界随机样本𝑁。
中心矩差异正则项 $CMD_K$ 被定义为 CMD 度量的经验估计，通过计算以下表达式来计算：
$$
CMD_K(X,Y) = \frac{1}{|b-a|}||E(X) - E(Y)||_2 + \sum_{k=2}^K\frac{1}{|b-a|^k}||C_k(X) - C_k(Y)||_2 \tag{7}
$$




其中，$E(𝑋) = \frac{1}{|X|}\sum_{x \in X}{x}$是样本𝑋的经验期望向量，而$C_k(X) = E((x-E(X))^k)$是所有坐标的𝑘^th^阶样本中心矩向量。



在我们的情况下，我们计算每对模态不变表示之间的CMD损失：
$$
L_{sim} = \frac{1}{3} \sum_{(m_1,m_2) \in {(l,a),(l,v),(a,v)}} CMD_K(h^c_{m_1},h^c_{m_2}) \tag{8}
$$



在这里，我们有两个重要观察结果：

（𝑖）我们选择CMD而不是KL散度或MMD，因为CMD是一种常用的度量方法[36]，并且能够显式匹配高阶矩，而无需计算昂贵的距离和核矩阵。

（𝑖𝑖）对抗性损失是相似性训练的另一种选择，其中鉴别器和共享编码器参与最小最大博弈。然而，我们选择CMD是因为它具有简单的公式。相比之下，对抗性训练需要为鉴别器添加额外的参数，并引入了额外的复杂性，如训练中的振荡[53]。



**3.5.2 $L_{diff}$ - 差异损失。**该损失旨在确保模态不变和模态特定的表示捕捉到输入的不同方面。通过对这两个表示之间施加软正交约束来实现非冗余性[5, 25, 47]。在一个训练批次的话语中，令$H^𝑐_𝑚$和$H^𝑝_𝑚$为矩阵，其行表示每个话语的模态𝑚的隐藏向量$h^𝑐_𝑚$和$h^𝑝_𝑚$。那么，对于该模态向量对的正交约束被计算为：


$$
||H^{c^T}_M H^p_m||^2_F \tag{9}
$$


这里，$||·||^2_𝐹$是平方弗罗贝尼乌斯范数。除了不变向量和特定向量之间的约束之外，我们还添加了模态特定向量之间的正交约束。然后，整体的差异损失被计算为：


$$
L_{diff} = \sum_{m \in \{l,v,a\}}||H^{c^T}_M H^p_m||^2_F +  \sum_{(m_1,m_2) \in {(l,a),(l,v),(a,v)}}  ||H^{c^T}_M H^p_m||^2_F \tag{9}
$$




